{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90e40b1",
   "metadata": {},
   "source": [
    "# Chapter 3: Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64429470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydata stack\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b08af",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MNIST <a name=\"mnist\"></a> \n",
    "\n",
    "The MNIST dataset is a classic set of 70,000 images of handwritten digits often used for testing classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b476e",
   "metadata": {},
   "source": [
    "## Training a Binary Classifier <a name=\"binaryclassifier\"></a>\n",
    "\n",
    "**Binary Classifier:** A classifier that distinguishes between 2 classes.\n",
    "\n",
    "**Stochastic Gradient Descent:** An efficient approach to fitting linear classifiers and regressors such as SVMs and Logistic Regressors\n",
    "- Works well for large and sparse datasets (efficient)\n",
    "- Technically an optimisation technique - the parameters determine the model\n",
    "- Sensitive to feature scaling and the order of training data (so shuffle it!)\n",
    "- `SGDClassifier` and `SGDRegressor` from `sklearn.linear_model`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c3b01c",
   "metadata": {},
   "source": [
    "## Performance Measures <a name=\"performancemeasures\"></a>\n",
    "\n",
    "**Accuracy:** The number of correctly classified samples as a proportion of the total.\n",
    "- Can be unhelpful as a metric - particularly if dataset is skewed\n",
    "- A dataset is *skewed* if some classes are much more frequent than others.\n",
    "\n",
    "**Confusion Matrix:** Matrix with $(A, B)$ entry the number of times class $A$ was classified as $B$\n",
    "- In a binary classification problem, row 0 gives how negative samples were classified, row 1 gives how positive samples were classified\n",
    "    - (0, 0): True negatives\n",
    "    - (0, 1): False positives\n",
    "    - (1, 0): False negatives\n",
    "    - (1, 1): True positives\n",
    "\n",
    "**Precision:** True positives as a proportion of things you classified as positive\n",
    "- A high value means that if you say something's positive it probably is - few false positives\n",
    "- Quality: you might call very few things positive\n",
    "- Ex: In information retrieval, precision is the fraction of retrieved documents that are relevant to the query\n",
    "- You can see this as $P(Y=1\\mid \\hat{Y}=1)$\n",
    "\n",
    "**Recall:** True positives as a proportion of things that are *actually* positive\n",
    "- A high value means that you rarely call something negative when it isn't - few false negatives\n",
    "- Also called *sensitivity*\n",
    "- Quantity: you might call everything positive\n",
    "- Ex: In information retrieval, recall is the fraction of the relevant documents that are successfully retrieved\n",
    "- You can see this as $P(\\hat{Y}=1\\mid Y=1)$\n",
    "\n",
    "**F1-Score:** The harmonic mean of precision and recall:\n",
    "\n",
    "\\begin{equation}\n",
    "    F_1 \n",
    "        = \\frac{2}{\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}}} \n",
    "        = 2 \\cdot \\frac{\\text{precision}\\cdot\\text{recall}}{\\text{precision} + \\text{recall}} \n",
    "        = \\frac{\\text{TP}}{\\text{TP}+\\frac{\\text{FN}+\\text{FP}}{2}}\n",
    "\\end{equation}\n",
    "- This favours classifiers that have similar precision and recall and penalises those where one is much higher\n",
    "- More generally, $F_{\\beta}$-score ($\\beta > 0$) is used if recall is considered $\\beta$ times more important than precision:\n",
    "\n",
    "\\begin{equation}\n",
    "    F_{\\beta}\n",
    "        = (1+\\beta^2) \\cdot \\frac{\\text{precision}\\cdot\\text{recall}}{\\beta^2\\cdot\\text{precision} + \\text{recall}} \n",
    "        = \\frac{\\text{TP}}{\\text{TP}+\\frac{\\beta^2\\text{FN}+\\text{FP}}{1+\\beta^2}}\n",
    "\\end{equation}\n",
    "\n",
    "**Decision Function:** A SGD classifier computes a score using a *decision function* and assigns a positive class if the score is greater than a *decision threshold*\n",
    "- Can be valuable to plot precision against recall for different threshold values using `precision_recall_curve`\n",
    "\n",
    "**Receiver Operation Characteristic (ROC) Curve:** Plots true positive rate against false positive rate for different threshold values\n",
    "- Use `roc_curve` to plot\n",
    "- False positive rate is 1 minus true negative rate (called *specificity*: $P(\\hat{Y}=0\\mid Y=0)$)\n",
    "- Want high true positive rate and low false negative rate\n",
    "- A random classifier would be diagonal from (0, 0) to (1, 1), we want curve to go as close to the top left as possible\n",
    "- The *area under the curve* (AUC) (using `roc_auc_score`) is a single value used to compare classifiers (greater is higher), but it doesn't capture the full trade-off between true positives and false positives\n",
    "\n",
    "**ROC vs Precision/Recall Curve:**\n",
    "- The PR Curve is generally preferred when the positive class is rare or when the positive class is more interesting than the negative class\n",
    "- [Here](https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves) is a rough explanation of why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bf7b83",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "\n",
    "**Multiclass Classifiers:** (or Multinomial Classifiers) can distinguish between more than 2 classes\n",
    "- Some classifiers can handle multiclass natively, for others you need to build multiclass out of multiple binary classifiers\n",
    "- *One-Versus-The-Rest (OvR):* (or one-versus-all) for each class you train a binary classifier on 'is the sample of this class'\n",
    "- *One-Versus-One (OvO):* train binary classifiers for each pair of classes (e.g. is this digit a 4 or a 5) and then choose the class with the most pairwise 'wins'\n",
    "- Draws in OvO are broken based on the total classification confidence among all pairs\n",
    "- For OvO you have to train many more classifiers, but each on a much smaller training set since you only include samples for the two classes being compared\n",
    "- Some algorithms (e.g. SVCs) scale badly with the training set so OvO is preferred, but for most OvR is preferred (Scikit-Learn automatically chooses the most appropriate, but you can override it)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d7d59-bd28-45a1-b4a6-21d59f1a5599",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "**Confusion Matrix**\n",
    "- Use confusion matrix to investigate numbers of errors by class and which classes are being misclassified as what\n",
    "- Can plot using `plt.matshow`\n",
    "- Valuable to normalise so you're looking at error rates, not absolute numbers, and remove the diagonal as rates there will be very high\n",
    "\n",
    "**Individual Errors**\n",
    "- Good (but time-consuming) to look at individaual errors\n",
    "- E.g. look at 10 examples each of where class A was correctly classified, was misidentified as class B, class B was correctly classified, and was misclassified as class A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d69136c-a000-4d25-a1ab-8673881608f1",
   "metadata": {},
   "source": [
    "## Multilabel Classification\n",
    "\n",
    "**Multilabel Classification:** Where a classifier has multiple labels for each input.\n",
    "- E.g. Identifying faces in a photo\n",
    "- This can be transformed into multiple binary classification problems, e.g. train a binary classifier for each potential label and output all those labels with a positive result\n",
    "- Some models suppose multilabel natively, e.g. K-Nearest Neighbours classification\n",
    "- To evaluate, you can calculate your favourite metric (e.g. F1-Score) for each label and then average. You may wish to do a weighted average according to how many samples have a particular label (the *support* of the label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b91ae4a-7edd-4970-b9bd-1739985f8436",
   "metadata": {},
   "source": [
    "## Multioutput Classification\n",
    "\n",
    "**Multioutput classification:** A generalisation of multilabel classification where the (multiple) labels can take more than 2 possible values.\n",
    "- Ex: Removing noise from images - output is lots of pixel, which each can take many values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe8485",
   "metadata": {},
   "source": [
    "## Code Samples <a name=\"codesamples\"></a>\n",
    "\n",
    "### Manual Cross-Validation\n",
    "\n",
    "The following code allows you to do cross-validation from scratch. We use the example of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b7a4650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training indices:  [1 2 4 5] Test indices:  [0 3]\n",
      "Proportion correct: 0.5\n",
      "Training indices:  [0 2 3 5] Test indices:  [1 4]\n",
      "Proportion correct: 0.0\n",
      "Training indices:  [0 1 3 4] Test indices:  [2 5]\n",
      "Proportion correct: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "# Initialise classifier\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Each split gives train and test indices for that fold that preserve the number of 0s and 1s in y\n",
    "skfolds = StratifiedKFold(n_splits=3)  # n_splits is n_folds for cross-validation\n",
    "\n",
    "for train_ix, test_ix in skfolds.split(X, y):  # Cycles through folds\n",
    "    clone_clf = clone(clf)  # clone to avoiding change actual classifier\n",
    "\n",
    "    print(\"Training indices: \", train_ix, \"Test indices: \", test_ix)\n",
    "    X_train_fold, y_train_fold = X[train_ix], y[train_ix]\n",
    "    X_test_fold, y_test_fold = X[test_ix], y[test_ix]\n",
    "\n",
    "    # Fit and print proportion correct\n",
    "    clf.fit(X_train_fold, y_train_fold)\n",
    "    y_pred = clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(\"Proportion correct: {:.1f}\".format(n_correct / len(y_test_fold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92021154",
   "metadata": {},
   "source": [
    "### Prediction Through Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b493e",
   "metadata": {},
   "source": [
    "**`cross_val_predict`:** Uses cross-validation to generate predictions\n",
    "- E.g. If using K-Fold it will split the data into K disjoint folds (so each observation lies in precisely one) and for each fold, train the model on the data without that fold and make predictions for that fold\n",
    "- You get predictions for the whole training set, but none of the observations are in their own training set\n",
    "- Of course this is slow because you have to train many times\n",
    "- You can also specific `method` to, e.g. use the decision function or get prediction probabilities, instead of just generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c270ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "# Initialise classifier\n",
    "clf = LogisticRegression()\n",
    "\n",
    "y_train_pred = cross_val_predict(clf, X, y, cv=3)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4339c-870f-443c-904e-32848a3b6460",
   "metadata": {},
   "source": [
    "### Execution Time Decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae62e6-4ae3-4c2b-94d8-ff384926a86f",
   "metadata": {},
   "source": [
    "Decorators are functions that add additional functionality to functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6ab40d0-7e38-4309-be88-194ca3cebc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 3.0 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "time.sleep(1)\n",
    "end_time = perf_counter()\n",
    "print(\"Run time: {:0.1f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426a724-4cf8-47d4-8754-37048d549a32",
   "metadata": {},
   "source": [
    "Here we are adding additional functionality to the `sleep` function. We can do this using a decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9355bea-7283-4656-9869-712e7eebb956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 1.0 seconds\n"
     ]
    }
   ],
   "source": [
    "def execution_time(func):\n",
    "    \n",
    "    def inner(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        print(\"Run time: {:0.1f} seconds\".format(end - start))\n",
    "    \n",
    "    \n",
    "    return inner\n",
    "\n",
    "\n",
    "def sleep_copy(seconds):\n",
    "    return sleep(seconds)\n",
    "\n",
    "\n",
    "sleep_decorated = run_time(sleep_copy)\n",
    "sleep_decorated(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d61bf-166f-4c22-9af2-6fc4deeba4ce",
   "metadata": {},
   "source": [
    "There is a shortcut for defining and decorating a function all in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fce92c5-d74e-446d-a659-53010ae8103a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 1.0 seconds\n"
     ]
    }
   ],
   "source": [
    "@execution_time\n",
    "def sleep_decorated(seconds):\n",
    "    return sleep(seconds)\n",
    "\n",
    "\n",
    "sleep_decorated(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
