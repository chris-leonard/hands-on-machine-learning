{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14564081-550d-4af1-b8ab-aa2ddd611323",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3709c05e-4dc0-4407-b5d6-7a656bac7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c4de19-da32-437a-88de-c41bc08e3e97",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "What is the approximate depth of a decision tree trained (without restrictions) on a training set with one million instances?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc49ab7c-972c-4a06-8c06-f019789657f1",
   "metadata": {},
   "source": [
    "No restrictions means that each leaf will have with 1 training instance. On average the tree will be balanced, so the number of children will double whenever we increase the depth. The question is how long can this continue until we run out of training instances. The answer is $log_2(m)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5771f6ba-afa8-421d-ac21-0dc803dd1d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.931568569324174\n"
     ]
    }
   ],
   "source": [
    "print(np.log2(1e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9712aa8-9f45-40fa-a448-532aa255b597",
   "metadata": {},
   "source": [
    "So the depth will be 20 on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fdbc02-968c-4c34-939d-a6723442a665",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Is a node's Gini impurity generally lower or greater than its parent's? It is *generally* lower/greater, or *always* lower/greater?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6fb5ee-f7f7-4adb-b9eb-eda49497d9de",
   "metadata": {},
   "source": [
    "The Gini impurity of a child is generally lower than its parent's because when splitting nodes we seek to minimise the weighted impurity of the children:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{m_{\\text{left}}}{m}G_{\\text{left}} + \\frac{m_{\\text{right}}}{m}G_{\\text{right}}.\n",
    "\\end{equation}\n",
    "\n",
    "However, one child may have higher impurity if this is compensated for in the weighted sum by a lower impurity in the other child."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897046bd-f426-47e6-8a03-541d3293d55e",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "If a decision tree is overfitting the training set, is it a good idea to try decreasing `max_depth`?\n",
    "\n",
    "---\n",
    "\n",
    "Yes. Decreasing `max_depth` simplifies the model and hence decreases the chance of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561dae5-f36d-4efa-ae1f-dce9247695e0",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "If a decision tree is underfitting the training set, is it a good idea to try scaling the input features?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba300bb-474b-46f3-9d3b-0abd81dbe3ac",
   "metadata": {},
   "source": [
    "No. The decision tree training algorithm is scale invarient, so this won't make any difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f7dea-4a40-4fb0-b81a-122ea1b0329f",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "If it takes one hour to train a decision tree on a training set contained 1 million instances, roughly how much time will it take to train another decision tree on a training set containing 10 million instances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e798da09-7796-404b-a292-3155b143dc88",
   "metadata": {},
   "source": [
    "The computational complexity of training a decision tree is $O(m \\log_2(m))$. So the increase in complexity will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e094824-5639-4ddb-ad97-80c9a9472f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.666666666666666"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cplx_old = 1e6 * np.log2(1e6)\n",
    "cplx_new = 1e7 * np.log2(1e7)\n",
    "cplx_new / cplx_old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848480e4-fadb-4939-b0e2-c6af64cade1c",
   "metadata": {},
   "source": [
    "So training the new tree will take approximately 11-12 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603b2891-1b98-4478-848b-38792984ac7c",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "If your training set contains 100,000 instances, will setting `presort=True` speed up training?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6942b0ac-42c5-489c-a47a-099f40bdafc4",
   "metadata": {},
   "source": [
    "No. With a training set that size, presorting will probably take more time than it will actually save in traning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53097f54-fe62-4d7a-90a1-042c5131faf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
