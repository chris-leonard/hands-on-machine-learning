{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f815b9f3-d1f6-4554-8e4a-fac4bf8493e5",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7bb9800-c695-40b9-a6a2-d0e0aec536c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (10, 8), \"font.size\": 18})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa73651-3639-4486-9123-0abd73e9a423",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91356451-1a2e-49b9-a3d9-7c137ff0d1e5",
   "metadata": {},
   "source": [
    "You can combine these models using hard or soft voting and it is possible that this will increase precision. It will be more likely that this will increase precision if the models are 'more' independent, for example if they are trained using very different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29c2ca4-4722-4f01-abf1-6025b6b94639",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "What is the difference between hard and soft voting classifiers?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e52a8-a2c8-493e-bc4f-47e406c4a688",
   "metadata": {},
   "source": [
    "Hard voting classifiers only use the outputs of the underlying predictors (choosing the most common class); soft voting classifiers take into account the underlying prediction probabilities in each predictor (selecting the class with the highest average). Soft voting only works if the constituent predictors give prediction probabilities, but if they do it generally performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850b8a6a-10f1-4c04-b40b-62b392807047",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa348b43-c428-488b-9089-52bdb8d287e1",
   "metadata": {},
   "source": [
    "Bagging, pasting, and random forests can be trained more quickly by distributing across multiple servers. This is because the training of the constituent predictors are independent. Boosting can't be distributed, because the training of each predictor depends on the outcome of the training of its predecessor. \n",
    "\n",
    "With stacking, training can be distributed *within* each layer but not *across* layers. That is, if a blender is trained on the outputs of three predictors, the training of the three predictors can be distributed, but the blender must be trained separately afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c1d2cd-c7b7-4074-847d-80c9b981ea0e",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "What is the benefit of out-of-bag evaluation?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f6efc1-9026-42d2-8fb2-500dc6eb1c0e",
   "metadata": {},
   "source": [
    "Out-of-bag evaluation allows you to perform validation on unseen data without needing a separate validation set or training the model multiple times (as in cross validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65a9e6-8c48-4345-a54d-c0136547cc2d",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "What makes extra-trees more random that regular random forests? How can the extra randomness help? Are extra-trees slower or faster than regular random forests?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2791879c-4dcf-47cc-8a69-fb754d427f06",
   "metadata": {},
   "source": [
    "With extra-trees the decision threshold at each node is selected randomly, instead of optimally for reducing impurity. The extra randomness generally reduces variance in the ensemble (though it may increase bias). Extra-trees are faster to train since the algorithm doesn't have to find the optimal threshold at each stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b17573-82ed-4a52-8c83-5105a9f41188",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "If you AdaBoost ensemble underfits the training data, which hyperparameters should you tweak and how?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1843c-63de-4072-be9e-a8dcbaf72351",
   "metadata": {},
   "source": [
    "You should increase the learning rate and/or the number of iterations. This will force the ensemble to compensate more for its errors and thus fit the data more closely. You could also change the hyperparameters of the underlying weak predictors to increase their variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f272e87-6a2a-4219-bd52-6a98857d15d9",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03a08f-2a16-4937-88b1-9b6c3ebb4027",
   "metadata": {},
   "source": [
    "You should decrease it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a8886a-c81c-4293-beea-45fcd1ce18af",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "\n",
    "Load the MNIST data and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). Then train various classifiers, such as a random forest, an extra-trees classifier, and an SVM classifier. Next, try to combine them into an ensemble that outperforms each individual classifier on the validation set, using soft or hard voting. Once you have found one, try it one the test set. How much better does it perform compared to the individual classifiers?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c47ee8-800f-4181-bae6-08419492137e",
   "metadata": {},
   "source": [
    "*I'll tackle this after I've completed 5.9 and so have a good SVM classifier*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d790117-963b-47f2-884f-c6b93c4e14c4",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "\n",
    "Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image's class. Train a classifier on this new training set. Congratulations, you have just trained a blender, and together with the classifiers it forms a stacking ensemble! Now evaluate the ensemble on the test set. For each image in the test set, make predictions withh all your classifiers, the feed the predictions to the blender to get the ensemble's predictions. How does it compare to the voting classifier you trained earlier?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df45af-5542-4eea-a5a7-2e1314af0b0f",
   "metadata": {},
   "source": [
    "*As above*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b204dac-2ed9-4e6c-b3bc-6bb37e3d0322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
