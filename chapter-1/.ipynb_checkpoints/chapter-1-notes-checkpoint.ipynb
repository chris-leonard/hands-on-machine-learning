{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3fa951",
   "metadata": {},
   "source": [
    "# Chapter 1: The Machine Learning Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d74cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e823b72a",
   "metadata": {},
   "source": [
    "## Types of Machine Learning System <a name=\"types\"></a>\n",
    "\n",
    "- *Accuracy:* proportion of correctly classified samples in classification\n",
    "- *Data mining:* applying ml techniques to dig into large amounts of data to discover patterns\n",
    "\n",
    "### Supervision <a name=\"supervision\"></a>\n",
    "\n",
    "**Supervised learning:** training data is labelled\n",
    "- Regression\n",
    "    - Multiple regression (multiple features)\n",
    "    - Uni/multi-variate regression (single/multiple outputs)\n",
    "- Classification\n",
    "\n",
    "**Unsupervised learning:** training data is unlabelled\n",
    "- Clustering\n",
    "- Visualisation: output 2D or 3D representation of data\n",
    "- Dimensionality reduction: simplify data without losing too much information (feature extraction)\n",
    "- Anomaly detection\n",
    "- Association rule learning: discover relations between attributes\n",
    "\n",
    "**Semi-supervised learning:** training data is partially labelled\n",
    "- Labelling friends in Facebook photos\n",
    "- Often a blend of supervised and unsupervised learning algorithms\n",
    "\n",
    "**Reinforcement learning:** agent pursues policy and learns according to rewards or penalties\n",
    "- DeepMind's AlphaGo\n",
    "\n",
    "### Batch / Online Learning <a name=\"batch\"></a>\n",
    "\n",
    "**Batch (offline) learning:** system is incapable of learning incrementally so it trained (offline) using all available data\n",
    "- Ineffective if you need to adapt to rapidly changing data\n",
    "- Takes a lot of computing power\n",
    "\n",
    "**Online learning:** train system incrementally by feeding it data sequentially (individually or in mini-batches)\n",
    "- Can discard new data instances after learning from them (saves space)\n",
    "- *Out-of-core* learning: use online learning to train systems on huge datasets (usually done offline)\n",
    "- *Learning rate:* how quickly system adapts to new data\n",
    "- If bad data is fed to system, performance will gradually decline\n",
    "\n",
    "### Instance / Model-Based Learning  <a name=\"instance\"></a>\n",
    "\n",
    "**Instance-based learning:**\n",
    "- Uses training data when evaluating new data point\n",
    "- 'Learns examples by heart'(?) and generalises to new data by using a similarity measure to compare with learned examples\n",
    "- Ex: K-Nearest Neighbor\n",
    "\n",
    "**Model-based learning:**\n",
    "- Training data is used to create model which is used to evaluate new data, training data is not used directly\n",
    "- *Utility function*: measures how good your model is\n",
    "- *Cost function*: measures how bad your model is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b95e662",
   "metadata": {},
   "source": [
    "## Main Challenges of Machine Learning <a name=\"challenges\"></a>\n",
    "\n",
    "**Insufficient training data:**\n",
    "- Reasonable algorithms may perform almost identically if there is enough data, but insufficient data is still common\n",
    "\n",
    "**Non-representative training data:**\n",
    "- *Sampling noise*: Non-representative data as a result of random chance (e.g. if sample is too small)\n",
    "- *Sampling bias\\*: Non-representative data as a result of flawed sampling\n",
    "\n",
    "**Poor quality data:**\n",
    "- E.g. missing features - possible responses include omitting this feature, omitting these instances, and approximating missing values\n",
    "\n",
    "**Irrelevant features:**\n",
    "- *Feature selection*: selecting best features to train on\n",
    "- *Feature extraction*: combining existing features to product better ones\n",
    "\n",
    "**Overfitting training data:**\n",
    "- *Overfitting*: the model performs well on the training data but doesn't generalise well\n",
    "- Possible solutions:\n",
    "    - Simplify the model by reducing number of parameters, number of attributes in training data, or by contraining the model\n",
    "    - Gather more training data\n",
    "    - Reduce noise in training data\n",
    "- *Regularisation*: constraining a model to make it simpler and reduce the risk of overfitting\n",
    "- *Hyperparameter*: a parameter of the learning algorithm, not the model. It is set prior to training and remains constant during training. The amount of regularisation is commonly controlled by hyperparameters.\n",
    "\n",
    "**Underfitting training data:**\n",
    "- *Underfitting*: when the model can't capture the complexity of the data\n",
    "- Possible solutions:\n",
    "    - Select a more complex/powerful model\n",
    "    - Improve the features (e.g. through engineering)\n",
    "    - Reduce constraints (e.g. by reducing regularisation hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60808e52",
   "metadata": {},
   "source": [
    "## Testing and Validation <a name=\"testing\"></a>\n",
    "\n",
    "- Recommended to split data into *training set* and *test set*\n",
    "- *Generalisation/out-of-sample error*: error rate on unseen data, estimated using the test set\n",
    "- If training error is low but generalisation error is high then model is overfitting training data\n",
    "- Common to use 80/20 train/test split but this depends on the size of dataset (you need enough in test set to get a good estimate of generalisation error)\n",
    "\n",
    "### Hyperparameter Tuning and Model Selection <a name=\"modelselection\"></a>\n",
    "\n",
    "- If you choose between models or select hyperparameters based on test set error then you have used the test set for training and test set error will not be a good estimate for generalisation error\n",
    "\n",
    "**Holdout Validation:**\n",
    "- Split data into training, validation (or development), and test sets\n",
    "- Train multiple models (with different hyperparameters) on training set, select model that performs best on validation set, retrain on training set + validation set, and estimate generalisation error using test set\n",
    "- This is effective if there is *lots* of data, but if the validation set is too small it won't give an accurate estimate of generalisation error in different models and if the reduced training set is too small then the models trained on it will perform significantly worse than those trained on the full training set.\n",
    "\n",
    "**Cross-validation:**\n",
    "- Perform validation many times with different small validation sets and average validation errors\n",
    "- Works better with less data, but you need to train models for each round of validation\n",
    "\n",
    "### Data Mismatch <a name=\"datamismatch\"></a>\n",
    "\n",
    "- You may have a large amount of data, but only a small part is representative of data that will be used in production\n",
    "- *Priority:* the data in validation and test sets should be as representative as possible of production data \n",
    "- Then if models perform poorly on the validation set, it may be because of a mismatch between training and validation sets\n",
    "- Solution:\n",
    "    - Hold some of the (unrepresentative) training data in a *train-dev set*\n",
    "    - Evaluate on train-dev set before validation set to determine if error is due to data mismatch or overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc732d",
   "metadata": {},
   "source": [
    "## No Free Lunch <a name=\"nofreelunch\"></a>\n",
    "- The choice of machine learning model is based on assumptions\n",
    "- *No Free Lunch Theorem:* Machine learning algorithms all perform the same when averaged over all possible problems\n",
    "- There is no such thing as a good machine learning algorithm, only an appropriate algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07000260",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834af94e",
   "metadata": {},
   "source": [
    "### Code Samples <a name=\"codesamples\"></a>\n",
    "\n",
    "#### Numpy r_ and c_ Functions <a name=\"r_\"></a>\n",
    "\n",
    "Short-hand for building up arrays quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2240d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 1D array with range specified by slice\n",
    "np.r_[0:5:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d59c13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [ 4,  5,  6],\n",
       "       [ 7,  8,  9],\n",
       "       [10, 11, 12]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "b = np.array([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Concatenate a and b along 0th axis\n",
    "np.r_[a, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e3c42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  7,  8,  9],\n",
       "       [ 4,  5,  6, 10, 11, 12]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate along 1st axis\n",
    "np.r_['1', a, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45bb8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd integer forces arrays to be 2-dimensional, then concatenate along 0th axis\n",
    "np.r_['0,2', [1,2,3], [4,5,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0371f313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "# 3rd integer specifies how to upgrade to upgrade to 2d\n",
    "# It gives the axis which will contain the start of the existing arrays\n",
    "# Default is -1\n",
    "print(np.r_['0,2,0', [1,2,3]].shape)\n",
    "print(np.r_['0,2,1', [1,2,3]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac82112d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.r_['0,2,0', [1,2,3], [4,5,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62b8eaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4],\n",
       "       [2, 5],\n",
       "       [3, 6]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c_[inputs] is short-hand for r_['1,2,0', inputs]\n",
    "# Useful for upgrading 1d to 2d as column vectors and concatenating horizontally\n",
    "np.c_[[1, 2, 3], [4, 5, 6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e56c0d6",
   "metadata": {},
   "source": [
    "### List of Machine Learning Algorithms <a name=\"mlalgorithms\"></a>\n",
    "\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- K-Nearest Neighbours\n",
    "- Neural Network\n",
    "    - Convolutional Neural Network (CNN)\n",
    "    - Recurrent Neural Network (RNN)\n",
    "    - Perceptron\n",
    "- Transformer\n",
    "- Support Vector Machine (SVM)\n",
    "    - Regression SVM\n",
    "    - One-Class SVM (anomaly detection)\n",
    "- Random Forest\n",
    "    - Regression Random Forest\n",
    "    - Decision Tree\n",
    "    - Isolation Forest (anomaly detection)\n",
    "- Deep Belief Network (DBN)    \n",
    "    - Restricted Boltzmann Machine (RBM)\n",
    "- K-meansClustering\n",
    "- DBSCAN\n",
    "- Hierarchical Cluster Analysis (HCA)\n",
    "- Principal Components Analysis (PCA)\n",
    "    - Kernel PCA\n",
    "- Locally Linear Embedding (LLE)\n",
    "- t-Distributed Stochastic Neighbour Embedding (t-SNE)\n",
    "- Assocation Rule Learning\n",
    "    - Apriori\n",
    "    - Eclat\n",
    "- Naive Bayes Classifier\n",
    "- Winnow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600c710",
   "metadata": {},
   "source": [
    "- Classification\n",
    "    - Stochastic Gradient Descent (SGD)\n",
    "    - Random Forest Classifier\n",
    "    - Support Vector Machine Classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939696d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
