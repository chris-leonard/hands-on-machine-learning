{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9a036a-bd35-4831-8caf-eb1325de0a93",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "What is the fundamental idea behind support vector machines?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aaf1b1-91d0-408e-b6f3-9841dfd30c95",
   "metadata": {},
   "source": [
    "To set a decision boundary that separates points in different classes by as great a margin as possible. When the data isn't linearly separable, we map it to a different feature space where it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15185745-0fe8-43d9-883a-f46878aeeb61",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "What is a support vector.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa810c8-669f-4a66-8a9b-e6162c8896c0",
   "metadata": {},
   "source": [
    "A support vector is a training instance that lies on one of the margins of the SVM *(or violates the margins)*. The model is entirely determined by the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343f149-05c4-4193-8449-c920c9483964",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Why is it important to scale the inputs when using SVMs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d35582-b84f-4793-a65d-08f1cf2f6365",
   "metadata": {},
   "source": [
    "SVMs are trained by maximising the distance between the decision boundary and training instances. If the inputs are on different scales then it will weight some features higher than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d7931-35cb-428f-8280-4619fb8ee2e7",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9d1c5-64c2-4ea2-9a4e-8d63c1233f98",
   "metadata": {},
   "source": [
    "The decision function $h(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x}+b$ is proportional to the distance to the decision boundary. If $\\text{sgn}(y)h(\\mathbf{x})=1$ then $\\mathbf{x}$ is a support vector, if its greater than one then it is outside the margins. Since SVM operate on the idea that the further you are from the decision boundary the 'more' you belong to a class, this can be used as a measure of confidence.\n",
    "\n",
    "SVM classifiers are not probabilistic models so can't output probabilities. The `SVC` class allows you to estimate probabilities, but this is an 'add-on' and apparently has theoretical issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de732fe3-0038-4e02-a09b-f81c5ccd9e54",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "\n",
    "Should you use the primal or the duel form of the SVM problem to train a model on a training set with millions of instances and hundreds of featues?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedcd003-a642-4086-a421-33868bd84d61",
   "metadata": {},
   "source": [
    "The primal form. The dual form is faster when the number of features exceeds the number of training instances.\n",
    "\n",
    "The primal problem involves optimising for $n+1$ variables ($\\mathbf{w}$ and $b$) with $m$ inequality constraints (assuming hard margin). The dual problem involves optimising for $m+1$ variables with $m$ inequality constraints. So the dual problem will be slower if $m\\gg n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b66b715-c5f1-45e8-9145-53854c425627",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "\n",
    "Say you've trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease $\\gamma$? What about $C$?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9d7d0-d2f2-4b7a-a77b-25e2a0a88271",
   "metadata": {},
   "source": [
    "The RBF kernel is of the form $k_i(\\mathbf{x})=\\exp(\\gamma \\lVert \\mathbf{x}^{(i)} - \\mathbf{x}\\rVert^2)$. Underfitting means these are probably too wide. To make them narrow we need to *increase $\\gamma$*.\n",
    "\n",
    "The hyperparameter $C$ determines how much we penalise margin violations. If we are underfitting then we probably aren't penalising them enough. So we also need to increase $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af01ef-a67f-4f7b-a01f-5a5dc343ece2",
   "metadata": {},
   "source": [
    "# Exercise 7\n",
    "\n",
    "How should you set the QP parameters ($\\mathbf{H}$, $\\mathbf{f}$, $\\mathbf{A}$, and $\\mathbf{b}$) so solve the soft margin linear SVM classifier problem using an off-the-shelf QP solver.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19888b6-2813-4f2f-98c9-bb0e841d0d86",
   "metadata": {},
   "source": [
    "We will seek to find an $(m+n+1)$-dimensional vector $\\mathbf{p}$ whose first $m$ elements are $\\zeta^{(1)}, \\ldots \\zeta^{(m)}$, whose $(m+1)$st entry is $b$,  and whose remaining $n$ components comprise $\\mathbf{w}$. Then:\n",
    "\n",
    "- let $\\mathbf{H}$ be the $(m+n+1)\\times(m+n+1)$-matrix with $I_n$ in the bottom right and all other entries zero;\n",
    "- let $\\mathbf{f}$ be the $(m+n+1)$-dimensional vector whose first $m$ entries are $C$ and has all other entries zero;\n",
    "- let $\\mathbf{b}$ be the $2m$-dimensional vector whose first $m$ entries are 1 and whose last $m$ entries are zero;\n",
    "- let $\\mathbf{A}$ be the $2m \\times(m+n+1)$-matrix whose left side has $I_m$ on the top and $-I_m$ on the bottom, and whose right hand side has $-t^{(i)}$ followed by $-t^{(i)}\\left(\\mathbf{x}^{(i)}\\right)^T$ for the first $m$ rows and 0s for the last $m$ rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff93692-e311-489e-9b1d-5167be154cf7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
