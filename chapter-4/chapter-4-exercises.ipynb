{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec8679c-deb0-4aed-a340-3ce1f76c2fa0",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93553fb-f54c-4df2-b744-16c51d11ee42",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Which Linear Regression algorithm can you use if you have a training set with millions of features?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd7fa2-6766-4f0a-a1fb-1409df631c41",
   "metadata": {},
   "source": [
    "Any of the gradient descent algorithms (batch, stochastic, mini-batch) will be relatively fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0333add7-37cc-49a3-aa74-9204185e874e",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Suppose the features in your training set have very different scales. Which algorithms suffer from this, and how? What can you do about this?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323e656-c033-4fb6-92ab-622e95c66229",
   "metadata": {},
   "source": [
    "The gradient descent algorithms suffer from this. Essentially, the learning rate is the same in all directions, so if the features have very different scales then it can't be set appropriately for all of them. It will either quickly converge on one axis and then crawl along a valley, or will jump around wildly on one axis.\n",
    "\n",
    "This can easily be resolved by feature scaling before training.\n",
    "\n",
    "*Also, regularisation may converge to suboptimal weights, since regularisation penalises large weights, features with smaller values will tend to be ignored.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4419c62-8145-451d-9763-bdd90b4c8ff0",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Can Gradient Descent algorithms get stuck in a local minimum with training a Logistic Regression model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74344d52-62cb-41b8-8cfc-a0f550a65cfe",
   "metadata": {},
   "source": [
    "No, the Logistic Regression cost function is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b431ca88-6546-4c0e-8c78-624dbdfcced5",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Do all Gradient Descent algorithms lead to the same model, provided you let them run long enough?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb6d885-b694-4940-b75b-8da3996f9775",
   "metadata": {},
   "source": [
    "Yes, provided that the learning rates are set such that the training algorithms converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757b43d-c4ac-4d72-8651-e00cd5e6693c",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ff89c-1ef2-45ca-8d49-d4cbc87729d3",
   "metadata": {},
   "source": [
    "If the validation error is consistently going up then the algorithm is likely not converging because the learning rate is too high - with each step you are 'jumping over' the minimum instead of getting closer. You can fix this by decreasing the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28caee22-7571-4ad8-8952-22c8dd73c4dd",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6597e435-4497-4505-8ec0-01e3751e0ba6",
   "metadata": {},
   "source": [
    "No. Mini-batch is naturally stochastic - it may go in the wrong direction for a step or two but later improve. You should let the algorithm run fully to find the global minimum for validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ffef06-e31d-421b-8e4d-09f89e3cfb7b",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution fastest? Which will actually converge? How can you make the others converge as well?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aadf09-1f76-4903-8628-8959cb30ae7d",
   "metadata": {},
   "source": [
    "Batch will reach the vicinity of the optimal solution in the fewest number of *steps*, but each step will take longer to compute. After computation time is taken into account, I expect that mini-batch would be fastest. *(It would be stochastic gradient descent or mini-batch with very small mini-batch size)*\n",
    "\n",
    "Only batch gradient descent will actually converge, but you can make stochastic and mini-batch converge by setting a suitable learning schedule that decreases the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333131c1-2b62-40e7-a8df-aa429ecd073b",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "\n",
    "Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e577713-ff3e-4cd7-9045-2f7a841bccdf",
   "metadata": {},
   "source": [
    "You are overfitting the training data. You could solve this by reducing the degree of the polynomials (simplifying the model), adding or increasig regularisation, or getting more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926842ba-4c95-4d96-bbeb-f706ae40a5e5",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "\n",
    "Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularisation parameter $\\alpha$ or reduce it?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599059b6-cfea-4876-b006-2ab1c3d60c6b",
   "metadata": {},
   "source": [
    "If the training and validation error are very similar then you are underfitting the training data so you have high *bias*. You should reduce the regularisation parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb73dbf-14c6-4e48-ae1b-140cbbf16e1f",
   "metadata": {},
   "source": [
    "## Exercise 10\n",
    "\n",
    "Why would you want to use:\n",
    "1. Ridge Regression instead of plain Linear Regression?\n",
    "2. Lasso instead of Ridge Regression?\n",
    "3. Elastic Net instead of Lasso?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7b23f7-81dd-4471-a48c-aaeaae20c2e3",
   "metadata": {},
   "source": [
    "1. Ridge Regression allows you to apply regularisation which reduces overfitting.\n",
    "2. Lasso tends to set coefficients equal to zero which performs feature selection as well as regularisation. This leads to a sparser model (which is generally perferred).\n",
    "3. Lasso tends to behave strangely when there are more features than training samples or when several features are strongly correlated. Also, Lasso is a special case of Elastic net at a specific hyperparameter value - adding this additional hyperparameter can only decrease validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c4f4fe-1f93-4721-bac2-bcb7f4c1224b",
   "metadata": {},
   "source": [
    "## Exercise 11\n",
    "\n",
    "Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regression classifier?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81f0f33-29c7-46d7-be54-d4d074e9b35d",
   "metadata": {},
   "source": [
    "Two logistic regression classifiers. Softmax regression is for multiclass classification, not multioutput."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b685d15-c86b-4348-95af-f433cc44d90c",
   "metadata": {},
   "source": [
    "## Exercise 12\n",
    "\n",
    "Implement Batch Gradient Descent with early stopping for Softmax Regression (without usinng Scikit-Learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039bddc8-668d-4cec-af76-53e84058d6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
