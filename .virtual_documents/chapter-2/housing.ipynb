# Pydata stack
import numpy as np
import pandas as pd
import scipy

# Plotting
from matplotlib import pyplot as plt
from pandas.plotting import scatter_matrix

# Scikit-Learn
# Data preparation
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.impute import SimpleImputer  # To deal with missing values
from sklearn.preprocessing import (
    OrdinalEncoder,
    OneHotEncoder,
)  # Turn categorical into numerical
from sklearn.preprocessing import MinMaxScaler, StandardScaler  # Feature scaling
from sklearn.base import (
    BaseEstimator,
    TransformerMixin,
)  # To create custom transformers
from sklearn.pipeline import Pipeline, make_pipeline  # Pipelines to chain estimators
from sklearn.compose import (
    ColumnTransformer,
    make_column_transformer,
    make_column_selector,
)  # To do estimators in parallel

from sklearn import set_config  # Displays HTML representation of composite estimators
set_config(display="diagram")

# Models
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

# Metrics
from sklearn.metrics import (
    mean_squared_error,
    mean_absolute_error,
    mean_absolute_percentage_error,
)  # metrics

# Cross validation/hyperparameter tuning
from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV

# Miscellaneous packages
from time import perf_counter  # To measure performance
import os  # For OS dependent functionality
import joblib  # To save arbitrary Python objects (e.g. models)
import tarfile  # To handle tgz files
import urllib.request  # To download data from url


# Note capitals for constants
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

# Function to retrieve data
def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)

    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()


# Call function to download data
fetch_housing_data()


# Function to load data
def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)


housing = load_housing_data()


housing.head(10)


housing.info()


housing.describe()


housing["ocean_proximity"].value_counts()


# Histogram of each numerical attribute
housing.hist(bins=50, figsize=(20, 15));


# Basic 80/20 split
# Random state ensures that same split can be replicated
train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)


# Turning median income from continuous to categorical variable
housing["income_cat"] = pd.cut(
    housing["median_income"],
    bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],
    labels=[1, 2, 3, 4, 5],
)

housing["income_cat"].hist();


housing.head()


split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

# Note this loop has only one iteration
# Syntax is necessary because of split class
# Makes more sense when n_splits > 1
for train_index, test_index in split.split(housing, housing["income_cat"]):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]


def income_cat_dist(df):
    print(df["income_cat"].value_counts() / len(df))


for df in [housing, strat_train_set, strat_test_set]:
    income_cat_dist(df)
    print("\n")


# Remove income_cat
for df in [strat_train_set, strat_test_set]:
    df.drop("income_cat", axis=1, inplace=True)


# Duplicate training set
housing = strat_train_set.copy()


# Scatter plot of coordinates of districts
housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1, figsize=(10, 7));


#
housing.plot(
    kind="scatter",
    x="longitude",
    y="latitude",
    alpha=0.4,
    s=housing["population"] / 100,  # Size of circles, over 100 for scaling
    label="population",
    c="median_house_value",
    cmap=plt.get_cmap("jet"),  # Predefined colour map from blue to red - a good default
    colorbar=True,  # Key for colours
    figsize=(10, 7),
    sharex=False,  # Fig bug with x-ticks and label not appearing
);


# Calculate correlation matrix
corr_matrix = housing.corr()


# Concentrate on median house value as this is what we want to predict
corr_matrix["median_house_value"].sort_values(ascending=False).round(2)


# Find the greatest correlations between features - might be good to remove highly correlated features
abs_corr_matrix = np.abs(corr_matrix)
max_corr = abs_corr_matrix[abs_corr_matrix != 1].stack().nlargest(10)
max_corr_feat = sorted(list({pair[0] for pair in max_corr.index}))

print("Features with the greatest correlations:")
corr_matrix.loc[max_corr_feat, max_corr_feat].round(2)


# Scatter matrix of attributes
scatter_matrix(housing, figsize=(20, 15));


# Can be better to concentrate on a few attributes
attributes = [
    "median_house_value",
    "median_income",
    "total_rooms",
    "housing_median_age",
]
scatter_matrix(housing[attributes], alpha=0.2, figsize=(15, 10));


# Focus on median income and median house value
housing.plot(
    kind="scatter",
    x="median_income",
    y="median_house_value",
    alpha=0.2,
    figsize=(12, 8),
);


# Try creating some new features from old
housing["rooms_per_household"] = housing["total_rooms"] / housing["households"]
housing["bedrooms_per_room"] = housing["total_bedrooms"] / housing["total_rooms"]
housing["population_per_household"] = housing["population"] / housing["households"]

# Take another look at correlation
corr_matrix = housing.corr()
corr_matrix["median_house_value"].sort_values(ascending=False)


# Get a clean copy of training set and separate into inputs and outputs
housing = strat_train_set.drop("median_house_value", axis=1)
housing_labels = strat_train_set["median_house_value"].copy()


# Resolve missing values by imputing the median

# df of numerical inputs
housing_num = housing.drop("ocean_proximity", axis=1)

# Instantiate estimator
imputer = SimpleImputer(strategy="median")

# Fit to training data
imputer.fit(housing_num)

# View medians
pd.DataFrame(
    np.r_[
        "0,2", imputer.statistics_
    ],  # learned parameters always have underscore suffix
    columns=housing_num.columns,
)


# Use trained estimator
X = imputer.transform(housing_num)  # Output is Numpy array
housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)


# Categorical features
housing_cat = housing[["ocean_proximity"]]


# Enumerate categories
ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
housing_cat_encoded[:10]


# Display encoding
pd.DataFrame(ordinal_encoder.categories_[0])


# Using one-hot encoding (encodes each possibility as a binary variable)
cat_encoder = OneHotEncoder()
housing_cat_1hot = cat_encoder.fit_transform(
    housing_cat
)  # Output has class SparseMatrix
housing_cat_1hot.toarray()[:5, :]


# Show encoding
cat_encoder.categories_


# Can also drop a single category to avoid colinearity
cat_encoder_drop = OneHotEncoder(drop="first")
housing_cat_1hot_drop = cat_encoder.fit_transform(housing_cat)
housing_cat_1hot_drop.toarray()[:5, :]


# Create a transformer to add extra 'combined' features

# Column indices - seems you want to input and output numpy arrays
rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6

# BaseEstimator base gives you get_params() and set_params() methods
# TransformerMixin gives you fit_transform() method
class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(
        self, add_bedrooms_per_room=True
    ):  # no *args or **kwargs, else BaseEstimator doesn't work
        self.add_bedrooms_per_room = (
            add_bedrooms_per_room  # arguments when instantiating class
        )

    def fit(self, X, y=None):
        return self  # must return self to fit with TransformerMixin

    def transform(self, X):
        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]
        population_per_household = X[:, population_ix] / X[:, households_ix]

        if self.add_bedrooms_per_room:
            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]

            return np.c_[
                X, rooms_per_household, population_per_household, bedrooms_per_room
            ]

        else:
            return np.c_[X, rooms_per_household, population_per_household]


# Testing
attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=True)

X = attr_adder.fit_transform(housing.values)
housing_extra_attribs = pd.DataFrame(
    X,
    columns=list(housing.columns)
    + ["rooms_per_household", "population_per_household", "bedrooms_per_room"],
    index=housing.index,
)

housing_extra_attribs.head(5)


# Method from parents BaseEstimator
attr_adder.get_params()


# Min max scaling on numerical data
scaler = MinMaxScaler()
scaler.fit(housing_num)

# Print some of the data
pd.DataFrame(
    np.c_[scaler.data_min_, scaler.data_max_, scaler.data_range_],
    columns=["min", "max", "range"],
    index=housing_num.columns,
)


# Scaled data
housing_num_scaled = pd.DataFrame(
    scaler.transform(housing_num), columns=housing_num.columns, index=housing_num.index
)

housing_num_scaled.head(5)


# Now try standardisation
standard_scaler = StandardScaler()
standard_scaler.fit(housing_num)

# Transform and print new mean and stdev
housing_num_std = pd.DataFrame(
    standard_scaler.transform(housing_num),
    columns=housing_num.columns,
    index=housing_num.index,
)
pd.DataFrame(
    np.c_[
        housing_num_std.mean(axis=0, skipna=True),
        housing_num_std.std(axis=0, skipna=True),
    ],
    columns=["mean", "std"],
    index=housing_num.columns,
).round(3)


# Create pipeline for numerical inputs
estimators = [
    ("imputer", SimpleImputer(strategy="median")),  # (name, estimator) pairs
    ("attribs_adder", CombinedAttributesAdder()),
    ("std_scaler", StandardScaler()),
]
num_pipeline = Pipeline(estimators)


# List of estimators
num_pipeline.steps


# Access constituent estimaters using index
num_pipeline[0]


# ...or label
num_pipeline["attribs_adder"]


# Params are labelled using syntax estimator__param
num_pipeline.get_params()


# make_pipeline is shorthand for Pipeline with labels generated automatically
make_pipeline(
    SimpleImputer(strategy="median"), CombinedAttributesAdder(), StandardScaler()
)


# Fitting pipeline does fit_transform on all estimators up to last where it fits
housing_num_tr = num_pipeline.fit_transform(housing_num)
housing_num_tr[:5, :]


# Apply different transformers to numerical and categorical features
num_attribs = list(housing_num)
cat_attribs = ["ocean_proximity"]

full_pipeline = ColumnTransformer(
    [
        ("num", num_pipeline, num_attribs),  # Triple (name, transformer, column labels)
        ("cat", OneHotEncoder(), cat_attribs),
    ]
)

housing_prepared = full_pipeline.fit_transform(housing)


# Display diagram of composite estimator
full_pipeline


# Can select columns by datatype using make_column_selector
trans = make_column_transformer(
    (num_pipeline, make_column_selector(dtype_include="float64")),
    (OneHotEncoder(), make_column_selector(dtype_include="object")),
)

np.array_equal(trans.fit_transform(housing), housing_prepared)


# To get a place for comparison, I want to check the base error rate
mean_predictions = np.array([housing_labels.mean()] * len(housing_labels))
base_rmse = mean_squared_error(mean_predictions, housing_labels, squared=False)
print("Base RMSE:", base_rmse.round(0))


# Train linear regression model
lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)  # Inputs (X, y)


# Try on a few samples
some_data = housing.iloc[:5]
some_labels = housing_labels.iloc[:5]
some_data_prepared = full_pipeline.transform(some_data)

print("Predictions:", lin_reg.predict(some_data_prepared).round(0))
print("Labels:", list(some_labels.round(0)))


# Measure performance using RMSE
housing_predictions = lin_reg.predict(housing_prepared)
lin_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)
round(lin_rmse, 0)  # Around half the base error rate


# Can also use other metrics
lin_mae = mean_absolute_error(housing_labels, housing_predictions)
lin_mape = mean_absolute_percentage_error(housing_labels, housing_predictions)

print("MAE: {}".format(lin_mae.round(0)))
print("MAPE: {}".format(lin_mape.round(2)))


# Try a more complex regression model
tree_reg = DecisionTreeRegressor(random_state=42)
tree_reg.fit(housing_prepared, housing_labels)

# Calculate RMSE
housing_predictions = tree_reg.predict(housing_prepared)
tree_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)
tree_rmse  # 0 because model is massively overfitting


# Utiliy function to show scores from cross validation
def display_scores(scores):
    print("scores:", scores.astype("int"))
    print("Mean:", int(scores.mean()))
    print("Standard Deviation:", int(scores.std()))  # How accurate is this estimate?


# Calculate cross validation error on tree regression model
scores = cross_val_score(
    tree_reg,
    housing_prepared,
    housing_labels,
    scoring="neg_root_mean_squared_error",  # All scoring assumes that high return values are better than lower
    cv=10,  # Why 10?
)
tree_rmse_scores = -scores
display_scores(tree_rmse_scores)


# And from linear regression model
lin_scores = cross_val_score(
    lin_reg,
    housing_prepared,
    housing_labels,
    scoring="neg_root_mean_squared_error",
    cv=10,
)
lin_rmse_scores = -lin_scores
display_scores(lin_rmse_scores)


# Commented out because it's slow to run
# # Dataframe to hold results
# mean_scores = pd.DataFrame(
#     columns=['n_folds', 'mean_score']
# )

# for n_folds in [2, 5, 10, 100, 500, 1000, 5000, 10000]:
#     scores = cross_val_score(
#         lin_reg,
#         housing_prepared,
#         housing_labels,
#         scoring='neg_root_mean_squared_error',
#         cv=n_folds
#     )

#     mean_scores = mean_scores.append({'n_folds' : n_folds, 'mean_score' : -scores.mean()}, ignore_index=True)


# mean_scores.plot(x='n_folds', y='mean_score', ylabel='mean_rmse');


# Now try a random forest
forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)
forest_reg.fit(housing_prepared, housing_labels)

housing_predictions = forest_reg.predict(housing_prepared)
forest_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)
print("Random Forest Training Error: ", forest_rmse.astype(int))


# CV scores
forest_scores = cross_val_score(
    forest_reg,
    housing_prepared,
    housing_labels,
    scoring="neg_root_mean_squared_error",
    cv=10,
)
forest_rmse_scores = -forest_scores

display_scores(forest_rmse_scores)


# Save models so we can just reload them next time with having to run everything
MODELS_FOLDER = "models"

# Removed the others as files were annoyingly big
joblib.dump(lin_reg, os.path.join(MODELS_FOLDER, "linear-regression-model.pkl"))
# joblib.dump(tree_reg, os.path.join(MODELS_FOLDER, 'tree-regression-model.pkl'))
# joblib.dump(forest_reg, os.path.join(MODELS_FOLDER, 'forest-regression-model.pkl'));


# Now we can just load them back in
lin_reg_loaded = joblib.load(os.path.join(MODELS_FOLDER, "linear-regression-model.pkl"))

# Test they are the same
housing_predictions = lin_reg.predict(housing_prepared)
housing_predictions_loaded = lin_reg_loaded.predict(housing_prepared)
np.array_equal(housing_predictions, housing_predictions_loaded)


# Initialise estimator
forest_reg = RandomForestRegressor()

# List parameters we can set
forest_reg.get_params()


# Parameters to try
param_grid = [
    {"n_estimators": [3, 10, 30], "max_features": [2, 3, 6, 8]},
    {"bootstrap": [False], "n_estimators": [3, 10], "max_features": [2, 3, 4]},
]

# Initialise grid search
grid_search = GridSearchCV(
    forest_reg,
    param_grid,
    scoring="neg_root_mean_squared_error",
    cv=5,  # Why now 5 instaed of 10?
    return_train_score=True,
    refit=True,
)

# Fit and time how long it takes
start_time = perf_counter()
grid_search.fit(housing_prepared, housing_labels)
end_time = perf_counter()
print("Grid search run time: {:0.1f} seconds".format(end_time - start_time))


# Get best estimator
grid_search.best_estimator_


# Show results
cv_res = grid_search.cv_results_
cv_res


def get_param_from_dict(dict, param_name, param_default=None):
    "Use with .apply() to get series of parameter values from series of params dicts"
    if dict.get(param_name) is not None:
        return dict.get(param_name)
    else:
        return param_default


# Get summary of results
cv_res_summary = pd.DataFrame(
    {
        "params": cv_res["params"],
        "mean_test_score": cv_res["mean_test_score"],
        "mean_train_score": cv_res["mean_train_score"],
    }
)

# Make test scores positive and round
cv_res_summary[["mean_test_score", "mean_train_score"]] = -cv_res_summary[
    ["mean_test_score", "mean_train_score"]
].astype(int)

# Split params column into individual parameter values
cv_res_summary["bootstrap_param"] = cv_res_summary["params"].apply(
    lambda dict: get_param_from_dict(dict, param_name="bootstrap", param_default=True)
)
cv_res_summary["max_features_param"] = cv_res_summary["params"].apply(
    lambda dict: get_param_from_dict(dict, param_name="max_features")
)
cv_res_summary["n_estimators_param"] = cv_res_summary["params"].apply(
    lambda dict: get_param_from_dict(dict, param_name="n_estimators")
)

# Clean up
cv_res_summary.drop(columns="params", inplace=True)
cv_res_summary = cv_res_summary[
    [
        "bootstrap_param",
        "max_features_param",
        "n_estimators_param",
        "mean_train_score",
        "mean_test_score",
    ]
]
cv_res_summary = cv_res_summary.sort_values(
    "mean_test_score", ascending=True
).reset_index(drop=True)

# Show results
cv_res_summary


# Take about 4 minutes to run

# # Initialise estimator
# forest_reg = RandomForestRegressor()

# # Set parameter distributions
# param_distributions = {
#     'bootstrap': scipy.stats.bernoulli(0.5), # Bernoulli true/false
#     'n_estimators': scipy.stats.randint(1, 100) # Random int from 1 to 100 uniformly
# }

# # Do random search
# random_search = RandomizedSearchCV(
#     forest_reg,
#     param_distributions,
#     n_iter=10,
#     cv=5,
#     scoring='neg_root_mean_squared_error',
#     return_train_score=True
# )

# start_time = perf_counter()
# random_search.fit(housing_prepared, housing_labels)
# end_time = perf_counter()
# print('Randomized search run time: {:0.1f} seconds'.format(end_time - start_time))

# # Get results
# cv_res = random_search.cv_results_

# cv_res_summary = pd.DataFrame(
#     {
#         'params' : cv_res['params'],
#         'mean_test_score' : cv_res['mean_test_score'],
#         'mean_train_score' : cv_res['mean_train_score'],
#     }
# )

# # Make test scores positive and round
# cv_res_summary[['mean_test_score', 'mean_train_score']] = -cv_res_summary[['mean_test_score', 'mean_train_score']].astype(int)

# # Split params column into individual parameter values
# cv_res_summary['bootstrap_param'] = cv_res_summary['params'].apply(
#     lambda dict: get_param_from_dict(dict, param_name='bootstrap')
# )
# cv_res_summary['n_estimators_param'] = cv_res_summary['params'].apply(
#     lambda dict: get_param_from_dict(dict, param_name='n_estimators')
# )

# # Clean up
# cv_res_summary.drop(columns='params', inplace=True)
# cv_res_summary = cv_res_summary[
#     ['bootstrap_param', 'n_estimators_param', 'mean_train_score', 'mean_test_score']
# ]
# cv_res_summary = cv_res_summary.sort_values('mean_test_score', ascending=True).reset_index(drop=True)

# # Show results
# cv_res_summary


# Fitted estimator describes feature importance - higher is better
# Q: Which models have this?
feature_importances = grid_search.best_estimator_.feature_importances_

# Get full list of attributes
extra_attribs = [
    "rooms_per_hhold",
    "pop_per_hhold",
    "bedrooms_per_room",
]  # Extra ones we added

cat_encoder = full_pipeline.named_transformers_["cat"]  # OneHotEncoder
cat_one_hot_attribs = list(cat_encoder.categories_[0])  # List of categories

attributes = num_attribs + extra_attribs + cat_one_hot_attribs

# Dataframe to store output
feature_importances_df = pd.DataFrame(
    {"feature": attributes, "importance": feature_importances}
)
feature_importances_df["importance"] = feature_importances_df["importance"].round(3)
feature_importances_df = feature_importances_df.sort_values(
    "importance", ascending=False
).reset_index(drop=True)

# Show values
feature_importances_df


# Get optimal model from grid search
final_model = grid_search.best_estimator_

# Prepare test data in same way as training (would be better to stick it all in one pipeline)
X_test = strat_test_set.drop("median_house_value", axis=1)  # Why not copy()?
y_test = strat_test_set["median_house_value"].copy()

X_test_prepared = full_pipeline.transform(X_test)

# Apply estimator
final_predictions = final_model.predict(X_test_prepared)

# Calculate error
final_rmse = mean_squared_error(y_test, final_predictions, squared=False)
print("Test RMSE: {:0.0f}".format(final_rmse))


# Generate 95% confidence interval - I am sceptical about this! What assumptions are being made?
confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2

np.sqrt(
    scipy.stats.t.interval(
        confidence,
        len(squared_errors) - 1,
        loc=squared_errors.mean(),
        scale=scipy.stats.sem(
            squared_errors
        ),  # This is standard error of sample mean as an estimator of the mean (i.e. s / sqrt(n-1))
    )
)



