# Pydata stack
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

plt.rcParams["figure.figsize"] = (20, 10)

# Misc
from random import randint

from sklearn.base import BaseEstimator

# scikit-learn
from sklearn.datasets import fetch_openml  # For open source ML sets
from sklearn.ensemble import RandomForestClassifier

# Models
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import (
    confusion_matrix,
    f1_score,
    precision_recall_curve,
    precision_score,
    recall_score,
    roc_auc_score,
    roc_curve,
)
from sklearn.model_selection import cross_val_predict, cross_val_score
from sklearn.multiclass import OneVsRestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC


mnist = fetch_openml("mnist_784", version=1)  # Output is a dictionary

# Load data into dataframes (in textbook they are arrays)
X, y = mnist["data"], mnist["target"]

# Print description
mnist["details"]


# Take a look at the data
X.head()


X.info()


X.describe()


y.head()


y = y.astype(int)


# Generate random index and print label
num_samples = 6000  # X.shape[0] # there are 6000 samples in training set
example_ix = randint(0, num_samples - 1)
print("Digit label: {}".format(y.iloc[example_ix]))

# Get pixel data and put in correct shape
example_digit = X.iloc[example_ix]
example_digit = example_digit.values  # Want underlying Numpy array
example_digit_image = example_digit.reshape(28, 28)

# Show image
plt.imshow(example_digit_image, cmap="binary")
plt.axis("off")


X_train, X_test, y_train, y_test = (
    X.iloc[:6000],
    X.iloc[6000:],
    y.iloc[:6000],
    y.iloc[6000:],
)


# Create binary 5 / not-5 label set
y_train_5 = y_train == 5
y_test_5 = y_test == 5


# Train classifier
sgd_clf = SGDClassifier(random_state=42)  # For reproducible output
sgd_clf.fit(X_train, y_train_5)

ex_pred = sgd_clf.predict([X_train.iloc[0]])[0]
print("First sample: {}".format(y_train.iloc[0]))
print("Prediction whether first sample is 5: {}".format(ex_pred))


cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")


class Never5Classifier(BaseEstimator):
    def fit(self, X, y=None):
        return self

    def predict(self, X):
        return np.zeros((len(X), 1), dtype=bool)


never_5_clf = Never5Classifier()
cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring="accuracy")


y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)
confusion_matrix(y_train_5, y_train_pred)


precision = precision_score(y_train_5, y_train_pred)
recall = recall_score(y_train_5, y_train_pred)

print("Precision: {:.2f}".format(precision))
print("Recall: {:.2f}".format(recall))


f1 = f1_score(y_train_5, y_train_pred)

print("F1-Score: {:.2f}".format(f1))


# get decision function scores
y_scores = cross_val_predict(
    sgd_clf, X_train, y_train_5, cv=3, method="decision_function"
)
precision, recall, thresholds = precision_recall_curve(y_train_5, y_scores)


def plot_decision_function_scores(precision, recall, thresholds):
    fig, ax = plt.subplots()

    f1 = 2 * precision * recall / (precision + recall)

    ax.plot(thresholds, precision[:-1], label="Precision")
    ax.plot(thresholds, recall[:-1], label="Recall")
    ax.plot(thresholds, f1[:-1], label="F1 Score")

    ax.legend()


plot_decision_function_scores(precision, recall, thresholds)


def plot_precision_against_recall(precision, recall):
    fig, ax = plt.subplots()

    ax.plot(precision, recall)


plot_precision_against_recall(precision, recall)


# plot ROC curve using false positive rate and true positive rate for various thresholds
fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)


def plot_roc_curve(fpr, tpr, label=None):
    fig, ax = plt.subplots()

    # plot roc curve
    ax.plot(fpr, tpr, label=label)

    # plot random roc curve of random classifier
    x_val = np.linspace(0, 1, 50)
    ax.plot(x_val, x_val, ls="--", color="k")


plot_roc_curve(fpr, tpr)


# get auc score
auc = roc_auc_score(y_train_5, y_scores)
print("AUC Score: {:.2f}".format(auc))


forest_clf = RandomForestClassifier(random_state=42)
y_probas_forest = cross_val_predict(
    forest_clf, X_train, y_train_5, cv=3, method="predict_proba"
)


y_probas_forest


y_probas_forest[example_ix, :]


y_scores_forest = y_probas_forest[:, 1]
fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)

plot_roc_curve(fpr_forest, tpr_forest, label="Random Forest")
plt.plot(fpr, tpr, label="SGD")
plt.legend()


# get auc score
auc = roc_auc_score(y_train_5, y_scores_forest)
print("AUC Score: {:.2f}".format(auc))


# SGD can handle multiclass natively
sgd_clf.fit(X_train, y_train)
sgd_clf.predict([example_digit])


# makes the classification based on decision function values for each class
sgd_clf.decision_function([example_digit])


# see classes in order
sgd_clf.classes_


# evaluate using accuracy (classes are balanced so this is now a good metric)
cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring="accuracy")


# scaling inputs improves performance even more
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring="accuracy")


svm_clf = SVC(decision_function_shape="ovo")
svm_clf.fit(X_train, y_train)
svm_clf.predict([example_digit])


# note that we manually forced it to return all pairwise decisions using decision_function_shape
example_digit_scores = svm_clf.decision_function([example_digit])
example_digit_scores


ovr_clf = OneVsRestClassifier(SVC())
ovr_clf.fit(X_train, y_train)


ovr_clf.predict([example_digit])


y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)
conf_mx = confusion_matrix(y_train, y_train_pred)
conf_mx


# plot confusion matrix
plt.matshow(conf_mx);


# normalise to get error rates and remove diagonals
row_sums = conf_mx.sum(axis=1)
norm_conf_mx = conf_mx / row_sums

np.fill_diagonal(norm_conf_mx, 0)
plt.matshow(norm_conf_mx);


y_train_large = y_train >= 7
y_train_odd = y_train % 2 == 1
y_multilabel = np.c_[y_train_large, y_train_odd]

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_multilabel)

knn_clf.predict([example_digit])


y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)
f1_score(y_multilabel, y_train_knn_pred, average="weighted") # weighted average



