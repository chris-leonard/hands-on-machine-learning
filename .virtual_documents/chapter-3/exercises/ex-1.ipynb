# Pydata stack
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

plt.rcParams["figure.figsize"] = (20, 10)

from random import randint
from time import perf_counter  # To measure performance

from sklearn.base import BaseEstimator

# scikit-learn
from sklearn.datasets import fetch_openml  # For open source ML sets
from sklearn.metrics import (
    confusion_matrix,
    accuracy_score
)
from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV, KFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import StandardScaler

from sklearn import set_config  # Displays HTML representation of composite estimators
set_config(display="diagram")


mnist = fetch_openml("mnist_784", version=1)

# load data into dataframes
X, y = mnist["data"], mnist["target"]

# use predefined train/test split
X_train, X_test, y_train, y_test = (
    X.iloc[:6000],
    X.iloc[6000:],
    y.iloc[:6000],
    y.iloc[6000:],
)


# Generate random index and print label
num_samples = X_train.shape[0]
example_ix = randint(0, num_samples - 1)
print("Digit label: {}".format(y.iloc[example_ix]))

# Get pixel data and put in correct shape
example_digit = X.iloc[example_ix]
example_digit = example_digit.values  # Want underlying Numpy array
example_digit_image = example_digit.reshape(28, 28)

# Show image
plt.imshow(example_digit_image, cmap="binary")
plt.axis("off");


class_counts = y_train.value_counts().sort_index()
class_pcts = 100 * class_counts / num_samples
class_pcts.round(1)


# total missing values
X_train.isnull().sum().sum()


# distribution of mean pixel values
X_train.mean().describe()


# distribution of variances amoung pixel values
X_train.var().describe()


scaler = StandardScaler()
scaler.fit(X_train)


full_pipeline = make_pipeline(StandardScaler())
X_train_prep = full_pipeline.fit_transform(X_train)


knc_clf = KNeighborsClassifier()
cv_score = cross_val_score(knc_clf, X_train_prep, y_train, cv=5, scoring="accuracy")
print("Mean Accuracy: {:.2f}".format(cv_score.mean()))


# available hyperparameters
knc_clf = KNeighborsClassifier()
knc_clf.get_params()


# hyperparams to try
param_grid = [
    {"n_neighbors": [1, 3, 10, 30, 100], "weights": ["uniform", "distance"]}
]

# Initialise grid search
grid_search = GridSearchCV(
    knc_clf,
    param_grid,
    scoring="accuracy",
    cv=5,
    return_train_score=True,
    refit=True,
)

# Fit and time how long it takes
start_time = perf_counter()
grid_search.fit(X_train_prep, y_train)
end_time = perf_counter()
print("Grid search run time: {:0.1f} seconds".format(end_time - start_time))


# full results
cv_results = grid_search.cv_results_
cv_results


def generate_grid_search_cv_summary(cv_results):
    cv_summary = pd.DataFrame(cv_results['params'])
    cv_summary["mean_train_score"] = cv_results["mean_train_score"].round(3)
    cv_summary["mean_test_score"] = cv_results["mean_test_score"].round(3)
    cv_summary = cv_summary.sort_values(by="mean_test_score", ascending=False)
    
    return cv_summary


cv_summary = generate_grid_search_cv_summary(cv_results)
cv_summary


# hyperparams to try
param_grid = [
    {"n_neighbors": [100, 300, 1000], "weights": ["distance"]}
]

# Initialise grid search
grid_search = GridSearchCV(
    knc_clf,
    param_grid,
    scoring="accuracy",
    cv=5,
    return_train_score=True,
    refit=True,
)

# Fit and time how long it takes
start_time = perf_counter()
grid_search.fit(X_train_prep, y_train)
end_time = perf_counter()
print("Grid search run time: {:0.1f} seconds".format(end_time - start_time))


cv_results = grid_search.cv_results_
cv_summary = generate_grid_search_cv_summary(cv_results)
cv_summary


# hyperparams to try
param_grid = [
    {"n_neighbors": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], "weights": ["uniform", "distance"]}
]

# Initialise grid search
grid_search = GridSearchCV(
    knc_clf,
    param_grid,
    scoring="accuracy",
    cv=5,
    return_train_score=True,
    refit=True,
)

# Fit and time how long it takes
start_time = perf_counter()
grid_search.fit(X_train_prep, y_train)
end_time = perf_counter()
print("Grid search run time: {:0.1f} seconds".format(end_time - start_time))


cv_results = grid_search.cv_results_
cv_summary = generate_grid_search_cv_summary(cv_results)
cv_summary


# get the best estimator
knc_clf = grid_search.best_estimator_
knc_clf


y_train_pred = cross_val_predict(knc_clf, X_train_prep, y_train, cv=5)
conf_mx = confusion_matrix(y_train, y_train_pred)
conf_mx


# noramlise to get error rates
row_sums = conf_mx.sum(axis=1)
norm_conf_mx = conf_mx / row_sums

np.fill_diagonal(norm_conf_mx, 0)
norm_conf_mx.round(3)


plt.matshow(norm_conf_mx);


# accuracy for each class
diag = [conf_mx[i, i] for i in range(conf_mx.shape[0])]
row_sums = conf_mx.sum(axis=1)
class_acc = diag / row_sums
class_acc.round(3)


kf = KFold(n_splits=num_samples) # to force non-stratified cross-validation

# Fit and time how long it takes
start_time = perf_counter()
knc_scores = cross_val_predict(knc_clf, X_train_prep, y_train, cv=kf, method="predict_proba")
end_time = perf_counter()
print("Predict time: {:0.1f} seconds".format(end_time - start_time))


knc_scores[:10, :]


np.argmax(knc_scores[:10, :], axis=1)


np.max(knc_scores[:10, :], axis=1)


max_probs = np.max(knc_scores, axis=1)

fig, ax = plt.subplots()
ax.hist(max_probs);


(max_probs == 1).sum() / num_samples


(max_probs >= 0.8).sum() / num_samples


def plot_proportions_above_thresholds(max_probs, bins=100):
    # bin according to how many points lie between 2 thresholds
    hist, bin_edges = np.histogram(max_probs, bins=bins, range=(0, 1))
    
    # cumulative sum for how many above threshold
    hist_cum = np.cumsum(hist)
    
    # normalise
    hist_cum_norm = hist_cum / len(max_probs)
    
    fig, ax = plt.subplots()
    
    plt.plot(bin_edges[:-1], hist_cum_norm)
    
    ax.set_xlabel('Thresholds')
    ax.set_ylabel('Proportion Above Threshold')
    
    results = 100 * np.c_[bin_edges[:-1], hist_cum_norm].round(3)
    
    return results


results = plot_proportions_above_thresholds(max_probs, bins=100)
ax = plt.gca();


results


X_train_prep_red = X_train_prep[(max_probs >= 0.75)]
y_train_red = y_train[(max_probs >= 0.75)]

X_train_prep_excl = X_train_prep[(max_probs < 0.75)]
y_train_excl = y_train[(max_probs < 0.75)]


# Is the sample still balanced?
class_counts = y_train_red.value_counts().sort_index()
class_pcts = 100 * class_counts / num_samples
class_pcts.round(1)


# hyperparams to try
param_grid = [
    {"n_neighbors": [3, 4, 5, 6, 7], "weights": ["uniform", "distance"]}
]

# Initialise grid search
grid_search = GridSearchCV(
    knc_clf,
    param_grid,
    scoring="accuracy",
    cv=5,
    return_train_score=True,
    refit=True,
)

# Fit and time how long it takes
start_time = perf_counter()
grid_search.fit(X_train_prep_red, y_train_red)
end_time = perf_counter()
print("Grid search run time: {:0.1f} seconds".format(end_time - start_time))


cv_results = grid_search.cv_results_
cv_summary = generate_grid_search_cv_summary(cv_results)
cv_summary


knc_clf = grid_search.best_estimator_
knc_clf


y_train_excl_pred = knc_clf.predict(X_train_prep_excl)
accuracy_score(y_train_excl, y_train_excl_pred)


# get reduced data
threshold = 0.4

X_train_prep_red = X_train_prep[(max_probs >= threshold)]
y_train_red = y_train[(max_probs >= threshold)]
num_samples_red = len(y_train_red)

X_train_prep_excl = X_train_prep[(max_probs < threshold)]
y_train_excl = y_train[(max_probs < threshold)]
num_samples_excl = len(y_train_excl)

# just choose some reasonable hyperparams
knc_clf = KNeighborsClassifier(n_neighbors=5, weights="distance")

# Do cross validation
cv_scores = cross_val_score(
    knc_clf,
    X_train_prep_red,
    y_train_red,
    scoring="accuracy",
    cv=5
)
mean_red_score = cv_scores.mean()
print("Mean CV Accuracy: {:.2f}".format(mean_red_score))

# now test on excluded samples
knc_clf.fit(X_train_prep_red, y_train_red)
y_train_excl_pred = knc_clf.predict(X_train_prep_excl)
excl_score = accuracy_score(y_train_excl, y_train_excl_pred)
print("Excl. Accuracy: {:.2f}".format(excl_score))

weighted_avg_score = (mean_red_score * num_samples_red + excl_score * num_samples_excl) / num_samples
print("Weighted Avg. Accuracy: {:.2f}".format(weighted_avg_score))



