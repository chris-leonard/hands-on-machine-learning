import numpy as np
import pandas as pd
from matplotlib import pyplot as plt


# simple example of training Linear Regression model on sample data.
from sklearn.linear_model import LinearRegression

# generate some sample data
X = 2 * np.random.rand(100, 1) # sampled from [0,1) with shape (100, 1)
y = 4 + 3 * X + np.random.rand(100, 1)

# fit linear regression model
lin_reg = LinearRegression()
lin_reg.fit(X, y)
print("Intercept: {:.2f}".format(lin_reg.intercept_[0]))
print("Coefficient: {:.2f}".format(lin_reg.coef_[0][0]))

# generate data to plot regression curve
X_new = np.linspace(0, 2, 101)
X_new = np.c_[X_new] # predict expects shape (m, 1), not (m)
y_pred = lin_reg.predict(X_new)

# plot curve against training data
fig, ax = plt.subplots(figsize = (15, 10))

ax.scatter(X, y, color="k", marker="x", label="Training Data")
ax.plot(X_new, y_pred, ls="--", color="red", label="Regression Curve")

ax.legend();


# you can manually calculate theta_hat using np.linalg.lstsq ('least squares')

# add bias (columns of 1s) to X
bias = np.ones(len(y))
X_bias = np.c_[bias, X]

theta, residuals, rank, s = np.linalg.lstsq(X_bias, y, rcond=None)

print("Intercept: {:.2f}".format(theta[0][0]))
print("Coefficient: {:.2f}".format(theta[1][0]))


# ...or by calculating the pseudo-inverse
X_inv = np.linalg.pinv(X_bias)
theta = X_inv.dot(y)
print("Intercept: {:.2f}".format(theta[0][0]))
print("Coefficient: {:.2f}".format(theta[1][0]))


# stochastic gradient descent for linear regression is implemented in SGDRegressor
from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor(
    max_iter=1000,
    tol=1e-3,
    penalty=None, # regularisation
    eta0=0.1 # initial linear rate
)
sgd_reg.fit(X, y.ravel()) # ravel changes y from (m,1) to (m,)

print("Intercept: {:.2f}".format(sgd_reg.intercept_[0]))
print("Coefficient: {:.2f}".format(sgd_reg.coef_[0]))


# simple example of polynomial regression on sample data
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# generate some sample data
m = 100
X = 6 * np.random.rand(m, 1) - 1 # sampled from [0,1) with shape (m, 1)
y = 0.5 * X**2 + X + 2 + np.random.rand(m, 1)

# fit linear regression model
lin_reg = LinearRegression()
lin_reg.fit(X, y)

# generate polynomial features
poly_feat = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_feat.fit_transform(X)

# fit polynomial regression model
poly_reg = LinearRegression()
poly_reg.fit(X_poly, y)
print("Intercept: {:.2f}".format(poly_reg.intercept_[0]))
print("Coefficient of x: {:.2f}".format(poly_reg.coef_[0][0]))
print("Coefficient of x^2: {:.2f}".format(poly_reg.coef_[0][1]))

def plot_regression_curves():
    # generate data
    X_new = np.linspace(-1, 5, 601)
    X_new = np.c_[X_new] # predict expects shape (m, 1), not (m)
    y_lin_pred = lin_reg.predict(X_new)
    
    X_new_poly = poly_feat.transform(X_new)
    y_poly_pred = poly_reg.predict(X_new_poly)

    # plot curves against training data
    fig, ax = plt.subplots(figsize = (15, 10))

    ax.scatter(X, y, color="k", marker="x", label="Training Data")
    ax.plot(X_new, y_lin_pred, ls="--", color="red", label="Linear Regression Curve")
    ax.plot(X_new, y_poly_pred, ls="--", color="blue", label="Polynomial Regression Curve")

    ax.legend();


plot_regression_curves()


import time

def computation_time(func):
    
    def inner(*args, **kwargs):
        start = time.perf_counter()
        func(*args, **kwargs)
        end = time.perf_counter()
        print("Computation time: {:.1f} seconds".format(end - start))
    
    
    return inner


def sleep_copy(seconds):
    return time.sleep(seconds)


sleep_decorated = computation_time(sleep_copy)
sleep_decorated(1)


@computation_time
def sleep_decorated(seconds):
    return time.sleep(seconds)


sleep_decorated(1)
