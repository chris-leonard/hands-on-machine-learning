import numpy as np
import pandas as pd
from matplotlib import pyplot as plt


# simple example of training Linear Regression model on sample data.
from sklearn.linear_model import LinearRegression

# generate some sample data
X = 2 * np.random.rand(100, 1)  # sampled from [0,1) with shape (100, 1)
y = 4 + 3 * X + np.random.rand(100, 1)

# fit linear regression model
lin_reg = LinearRegression()
lin_reg.fit(X, y)
print("Intercept: {:.2f}".format(lin_reg.intercept_[0]))
print("Coefficient: {:.2f}".format(lin_reg.coef_[0][0]))

# generate data to plot regression curve
X_new = np.linspace(0, 2, 101)
X_new = np.c_[X_new]  # predict expects shape (m, 1), not (m)
y_pred = lin_reg.predict(X_new)

# plot curve against training data
fig, ax = plt.subplots(figsize=(15, 10))

ax.scatter(X, y, color="k", marker="x", label="Training Data")
ax.plot(X_new, y_pred, ls="--", color="red", label="Regression Curve")

ax.legend();


# you can manually calculate theta_hat using np.linalg.lstsq ('least squares')

# add bias (columns of 1s) to X
bias = np.ones(len(y))
X_bias = np.c_[bias, X]

theta, residuals, rank, s = np.linalg.lstsq(X_bias, y, rcond=None)

print("Intercept: {:.2f}".format(theta[0][0]))
print("Coefficient: {:.2f}".format(theta[1][0]))


# ...or by calculating the pseudo-inverse
X_inv = np.linalg.pinv(X_bias)
theta = X_inv.dot(y)
print("Intercept: {:.2f}".format(theta[0][0]))
print("Coefficient: {:.2f}".format(theta[1][0]))


# stochastic gradient descent for linear regression is implemented in SGDRegressor
from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor(
    max_iter=1000,
    tol=1e-3,
    penalty=None,  # regularisation
    eta0=0.1,  # initial linear rate
)
sgd_reg.fit(X, y.ravel())  # ravel changes y from (m,1) to (m,)

print("Intercept: {:.2f}".format(sgd_reg.intercept_[0]))
print("Coefficient: {:.2f}".format(sgd_reg.coef_[0]))


# simple example of polynomial regression on sample data
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# generate some sample data
n_samples = 200
X = 6 * np.random.rand(n_samples, 1) - 1
y = 0.5 * X**2 + X + 2 + np.random.rand(n_samples, 1)

# fit linear regression model
lin_reg = LinearRegression()
lin_reg.fit(X, y)

# generate polynomial features
poly_feat = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_feat.fit_transform(X)

# fit polynomial regression model
poly_reg = LinearRegression()
poly_reg.fit(X_poly, y)
print("Intercept: {:.2f}".format(poly_reg.intercept_[0]))
print("Coefficient of x: {:.2f}".format(poly_reg.coef_[0][0]))
print("Coefficient of x^2: {:.2f}".format(poly_reg.coef_[0][1]))


def plot_regression_curves():
    # generate data
    X_new = np.linspace(-1, 5, 601)
    X_new = np.c_[X_new]  # predict expects shape (m, 1), not (m)
    y_lin_pred = lin_reg.predict(X_new)

    X_new_poly = poly_feat.transform(X_new)
    y_poly_pred = poly_reg.predict(X_new_poly)

    # plot curves against training data
    fig, ax = plt.subplots(figsize=(15, 10))

    ax.scatter(X, y, color="k", marker="x", label="Training Data")
    ax.plot(X_new, y_lin_pred, ls="--", color="red", label="Linear Regression Curve")
    ax.plot(
        X_new, y_poly_pred, ls="--", color="blue", label="Polynomial Regression Curve"
    )

    ax.legend()


plot_regression_curves()


# learning curves for linear model
from sklearn.model_selection import learning_curve


def plot_learning_curves(estimator, train_sizes, y_top=None):
    train_sizes, train_scores, valid_scores = learning_curve(
        estimator,
        X,
        y,
        train_sizes=train_sizes,  # this has to be kwarg or error
        cv=5,
        scoring="neg_mean_squared_error",
    )

    avg_train_mse = -train_scores.mean(axis=1)
    avg_valid_mse = -valid_scores.mean(axis=1)

    fig, ax = plt.subplots(figsize=(15, 8))

    ax.plot(train_sizes, avg_train_mse, marker="x", label="Training Set")
    ax.plot(train_sizes, avg_valid_mse, marker="x", label="Validation Set")

    ax.set_xlabel("Training Set Size")
    ax.set_ylabel("Mean Squared Error")

    ax.set_title("Learning Curve")

    ax.set_ylim(bottom=0, top=y_top)

    ax.legend()


train_sizes = np.linspace(
    0.03, 0.8, 50
)  # can also provide int sizes of training sets to use
lin_reg = LinearRegression()
plot_learning_curves(lin_reg, train_sizes, y_top=3)


# learning curves for degree 10 polynomial regression
from sklearn.pipeline import make_pipeline

poly10_reg = make_pipeline(
    PolynomialFeatures(degree=10, include_bias=False), LinearRegression()
)
train_sizes = np.linspace(
    0.01, 0.8, 50
)  # can also provide int sizes of training sets to use
plot_learning_curves(poly10_reg, train_sizes, y_top=0.2)


# finally for degree 2
poly2_reg = make_pipeline(
    PolynomialFeatures(degree=2, include_bias=False), LinearRegression()
)
train_sizes = np.linspace(
    0.01, 0.8, 50
)  # can also provide int sizes of training sets to use
plot_learning_curves(poly2_reg, train_sizes, y_top=0.15)


# ridge regressionn using the closed form solutionn
from sklearn.linear_model import Ridge

ridge_reg = Ridge(alpha=1, solver="cholesky") # why this solver?
ridge_reg.fit(X, y)

# using stochastic gradient descent
sgd_reg = SGDRegressor(penalty="l2") # how can we specify alpha?
sgd_reg.fit(X, y.ravel());


# lasso model
from sklearn.linear_model import Lasso

lasso_reg = Lasso(alpha=0.1) # is this just the initial value - does it automatically apply learning schedule?
lasso_reg.fit(X, y)

# can also apply using stochastic gradient descent
sgd_reg_lasso = SGDRegressor(penalty="l1") # how can we specify alpha?
sgd_reg_lasso.fit(X, y.ravel());


# elastic net
from sklearn.linear_model import ElasticNet

elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5) # l1_ratio is r above
elastic_net.fit(X, y);


import time


def computation_time(func):
    def inner(*args, **kwargs):
        start = time.perf_counter()
        func(*args, **kwargs)
        end = time.perf_counter()
        print("Computation time: {:.1f} seconds".format(end - start))

    return inner


def sleep_copy(seconds):
    return time.sleep(seconds)


sleep_decorated = computation_time(sleep_copy)
sleep_decorated(1)


@computation_time
def sleep_decorated(seconds):
    return time.sleep(seconds)


sleep_decorated(1)
