import numpy as np
import pandas as pd
from matplotlib import pyplot as plt


# simple example of training Linear Regression model on sample data.
from sklearn.linear_model import LinearRegression

# generate some sample data
X = 2 * np.random.rand(100, 1)  # sampled from [0,1) with shape (100, 1)
y = 4 + 3 * X + np.random.rand(100, 1)

# fit linear regression model
lin_reg = LinearRegression()
lin_reg.fit(X, y)
print("Intercept: {:.2f}".format(lin_reg.intercept_[0]))
print("Coefficient: {:.2f}".format(lin_reg.coef_[0][0]))

# generate data to plot regression curve
X_new = np.linspace(0, 2, 101)
X_new = np.c_[X_new]  # predict expects shape (m, 1), not (m)
y_pred = lin_reg.predict(X_new)

# plot curve against training data
fig, ax = plt.subplots(figsize=(15, 10))

ax.scatter(X, y, color="k", marker="x", label="Training Data")
ax.plot(X_new, y_pred, ls="--", color="red", label="Regression Curve")

ax.legend();


# you can manually calculate theta_hat using np.linalg.lstsq ('least squares')

# add bias (columns of 1s) to X
bias = np.ones(len(y))
X_bias = np.c_[bias, X]

theta, residuals, rank, s = np.linalg.lstsq(X_bias, y, rcond=None)

print("Intercept: {:.2f}".format(theta[0][0]))
print("Coefficient: {:.2f}".format(theta[1][0]))


# ...or by calculating the pseudo-inverse
X_inv = np.linalg.pinv(X_bias)
theta = X_inv.dot(y)
print("Intercept: {:.2f}".format(theta[0][0]))
print("Coefficient: {:.2f}".format(theta[1][0]))


# stochastic gradient descent for linear regression is implemented in SGDRegressor
from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor(
    max_iter=1000,
    tol=1e-3,
    penalty=None,  # regularisation
    eta0=0.1,  # initial linear rate
)
sgd_reg.fit(X, y.ravel())  # ravel changes y from (m,1) to (m,)

print("Intercept: {:.2f}".format(sgd_reg.intercept_[0]))
print("Coefficient: {:.2f}".format(sgd_reg.coef_[0]))


# simple example of polynomial regression on sample data
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# generate some sample data
n_samples = 200
X = 6 * np.random.rand(n_samples, 1) - 1
y = 0.5 * X**2 + X + 2 + np.random.rand(n_samples, 1)

# fit linear regression model
lin_reg = LinearRegression()
lin_reg.fit(X, y)

# generate polynomial features
poly_feat = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_feat.fit_transform(X)

# fit polynomial regression model
poly_reg = LinearRegression()
poly_reg.fit(X_poly, y)
print("Intercept: {:.2f}".format(poly_reg.intercept_[0]))
print("Coefficient of x: {:.2f}".format(poly_reg.coef_[0][0]))
print("Coefficient of x^2: {:.2f}".format(poly_reg.coef_[0][1]))


def plot_regression_curves():
    # generate data
    X_new = np.linspace(-1, 5, 601)
    X_new = np.c_[X_new]  # predict expects shape (m, 1), not (m)
    y_lin_pred = lin_reg.predict(X_new)

    X_new_poly = poly_feat.transform(X_new)
    y_poly_pred = poly_reg.predict(X_new_poly)

    # plot curves against training data
    fig, ax = plt.subplots(figsize=(15, 10))

    ax.scatter(X, y, color="k", marker="x", label="Training Data")
    ax.plot(X_new, y_lin_pred, ls="--", color="red", label="Linear Regression Curve")
    ax.plot(
        X_new, y_poly_pred, ls="--", color="blue", label="Polynomial Regression Curve"
    )

    ax.legend()


plot_regression_curves()


# learning curves for linear model
from sklearn.model_selection import learning_curve


def plot_learning_curves(estimator, train_sizes, y_top=None):
    train_sizes, train_scores, valid_scores = learning_curve(
        estimator,
        X,
        y,
        train_sizes=train_sizes,  # this has to be kwarg or error
        cv=5,
        scoring="neg_mean_squared_error",
    )

    avg_train_mse = -train_scores.mean(axis=1)
    avg_valid_mse = -valid_scores.mean(axis=1)

    fig, ax = plt.subplots(figsize=(15, 8))

    ax.plot(train_sizes, avg_train_mse, marker="x", label="Training Set")
    ax.plot(train_sizes, avg_valid_mse, marker="x", label="Validation Set")

    ax.set_xlabel("Training Set Size")
    ax.set_ylabel("Mean Squared Error")

    ax.set_title("Learning Curve")

    ax.set_ylim(bottom=0, top=y_top)

    ax.legend()


train_sizes = np.linspace(
    0.03, 0.8, 50
)  # can also provide int sizes of training sets to use
lin_reg = LinearRegression()
plot_learning_curves(lin_reg, train_sizes, y_top=3)


# learning curves for degree 10 polynomial regression
from sklearn.pipeline import make_pipeline

poly10_reg = make_pipeline(
    PolynomialFeatures(degree=10, include_bias=False), LinearRegression()
)
train_sizes = np.linspace(
    0.01, 0.8, 50
)  # can also provide int sizes of training sets to use
plot_learning_curves(poly10_reg, train_sizes, y_top=0.2)


# finally for degree 2
poly2_reg = make_pipeline(
    PolynomialFeatures(degree=2, include_bias=False), LinearRegression()
)
train_sizes = np.linspace(
    0.01, 0.8, 50
)  # can also provide int sizes of training sets to use
plot_learning_curves(poly2_reg, train_sizes, y_top=0.15)


# ridge regressionn using the closed form solutionn
from sklearn.linear_model import Ridge

ridge_reg = Ridge(alpha=1, solver="cholesky")  # why this solver?
ridge_reg.fit(X, y)

# using stochastic gradient descent
sgd_reg = SGDRegressor(penalty="l2")  # how can we specify alpha?
sgd_reg.fit(X, y.ravel());


# lasso model
from sklearn.linear_model import Lasso

lasso_reg = Lasso(
    alpha=0.1
)  # is this just the initial value - does it automatically apply learning schedule?
lasso_reg.fit(X, y)

# can also apply using stochastic gradient descent
sgd_reg_lasso = SGDRegressor(penalty="l1")  # how can we specify alpha?
sgd_reg_lasso.fit(X, y.ravel());


# elastic net
from sklearn.linear_model import ElasticNet

elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio is r above
elastic_net.fit(X, y);


from sklearn import datasets
from sklearn.utils import shuffle

# import iris sample data
iris = datasets.load_iris()

X = pd.DataFrame(data=iris["data"], columns=iris["feature_names"])
y = iris["target"]

# shuffle since data is ordered by class
X, y = shuffle(X, y)

# restrict to binary problem
y_virg = (y == 2).astype(int)


# binary logistic regression on just petal width
from sklearn.linear_model import LogisticRegression

X_petal_width = (
    X["petal width (cm)"].to_numpy().reshape(-1, 1)
)  # changes to shape (*, 1)

log_reg_petal_width = LogisticRegression(penalty="none")  # no regularisation
log_reg_petal_width.fit(X_petal_width, y_virg)


def plot_1d_decision_boundary():
    fig, ax = plt.subplots(figsize=(15, 8))

    x_plot = np.linspace(X_petal_width.min(), X_petal_width.max(), 100).reshape(-1, 1)
    y_plot = log_reg_petal_width.predict_proba(x_plot)  # (m, 2) array with P(0), P(1)

    # decision curves
    ax.plot(x_plot, y_plot[:, 1], color="blue")
    ax.plot(x_plot, y_plot[:, 0], ls="--", color="orange")

    # data points
    ax.scatter(
        X_petal_width[y_virg == 1],
        y_virg[y_virg == 1],
        marker="^",
        color="blue",
        label="Iris Virginica",
    )
    ax.scatter(
        X_petal_width[y_virg == 0],
        y_virg[y_virg == 0],
        marker="s",
        color="orange",
        label="Not Iris Virginica",
    )

    # decision boundary is where linear term is 0
    decision_boundary = -log_reg_petal_width.intercept_ / log_reg_petal_width.coef_
    ax.axvline(decision_boundary, color="k", ls=":", label="Decision Boundary")

    ax.set_xlabel("Petal Width(cm)")
    ax.set_ylabel("Probability")

    ax.set_title("Logistic Regression Estimated Probabilities")

    ax.legend()


plot_1d_decision_boundary()


# binary logistic regression on just petal width and length
from sklearn.linear_model import LogisticRegression

X_petal = X[["petal length (cm)", "petal width (cm)"]]

log_reg_petal = LogisticRegression(penalty="none")  # no regularisation
log_reg_petal.fit(X_petal, y_virg)


def plot_2d_decision_boundary():
    fig, ax = plt.subplots(figsize=(15, 8))

    # plot decision boundary - points where linear term is zero a+bx+cy = 0
    x_plot = np.linspace(
        X_petal["petal length (cm)"].min(), X_petal["petal length (cm)"].max(), 100
    )
    y_plot = -log_reg_petal.coef_[0][0] * x_plot - log_reg_petal.intercept_
    y_plot /= log_reg_petal.coef_[0][1]

    ax.plot(x_plot, y_plot, color="k", ls="--", label="Decision Boundary")

    # data points
    ax.scatter(
        X_petal.loc[y_virg == 1, "petal length (cm)"],
        X_petal.loc[y_virg == 1, "petal width (cm)"],
        marker="^",
        color="blue",
        label="Iris Virginica",
    )
    ax.scatter(
        X_petal.loc[y_virg == 0, "petal length (cm)"],
        X_petal.loc[y_virg == 0, "petal width (cm)"],
        marker="s",
        color="orange",
        label="Not Iris Virginica",
    )

    ax.set_xlabel("Petal Length (cm)")
    ax.set_ylabel("Petal Width (cm)")

    ax.set_title("Logistic Regression on Petal Length - Decision Boundary")

    ax.legend()


plot_2d_decision_boundary()


# softmax regression on petal length with all 3 classes
softmax_reg = LogisticRegression(
    multi_class="multinomial", penalty="none"  # uses OvR by default,
)
softmax_reg.fit(X_petal, y)


def plot_multinomial_decision_boundaries():
    fig, ax = plt.subplots(figsize=(15, 8))

    # plot all the decision boundaries
    x_plot = np.linspace(
        X_petal["petal length (cm)"].min(), X_petal["petal length (cm)"].max(), 100
    )

    def solve_linear_equation(intercept, coef, x):
        "Solve equation of the form intercept + coef[0]*x + coef[1]*y=0 for y"
        y = -coef[0] * x - intercept
        y /= coef[1]

        return y

    y_plot = np.zeros((3, len(x_plot)))
    row = 0
    for i in range(3):
        for j in range(i + 1, 3):
            y_plot[row, :] = solve_linear_equation(
                softmax_reg.intercept_[i] - softmax_reg.intercept_[j],
                softmax_reg.coef_[i, :] - softmax_reg.coef_[j, :],
                x_plot,
            )

            if row == 0:
                ax.plot(
                    x_plot,
                    y_plot[row, :],
                    color="k",
                    ls="--",
                    label="Decision Boundaries",
                )
            else:
                ax.plot(x_plot, y_plot[row, :], color="k", ls="--")

            row += 1

    # colour areas with class - this is manual and doesn't generalise
    ax.fill_between(
        x_plot, y_plot[0, :].min(), y_plot[0, :], facecolor="orange", alpha=0.3
    )
    ax.fill_between(x_plot, y_plot[0, :], y_plot[2, :], facecolor="blue", alpha=0.3)
    ax.fill_between(
        x_plot, y_plot[2, :], y_plot[2, :].max(), facecolor="green", alpha=0.3
    )

    # data points
    ax.scatter(
        X_petal.loc[y == 0, "petal length (cm)"],
        X_petal.loc[y == 0, "petal width (cm)"],
        marker="o",
        color="orange",
        label="Iris Setosa",
    )
    ax.scatter(
        X_petal.loc[y == 1, "petal length (cm)"],
        X_petal.loc[y == 1, "petal width (cm)"],
        marker="s",
        color="blue",
        label="Iris Versicolor",
    )
    ax.scatter(
        X_petal.loc[y == 2, "petal length (cm)"],
        X_petal.loc[y == 2, "petal width (cm)"],
        marker="^",
        color="green",
        label="Iris Virginica",
    )

    ax.set_xlabel("Petal Length (cm)")
    ax.set_ylabel("Petal Width (cm)")

    ax.set_title("Softmax Regression on Petal Length - Decision Boundaries")

    ax.set_ylim(0, 3)

    ax.legend()


plot_multinomial_decision_boundaries()


import time


def computation_time(func):
    def inner(*args, **kwargs):
        start = time.perf_counter()
        func(*args, **kwargs)
        end = time.perf_counter()
        print("Computation time: {:.1f} seconds".format(end - start))

    return inner


def sleep_copy(seconds):
    return time.sleep(seconds)


sleep_decorated = computation_time(sleep_copy)
sleep_decorated(1)


@computation_time
def sleep_decorated(seconds):
    return time.sleep(seconds)


sleep_decorated(1)
